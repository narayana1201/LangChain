{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Requirements Installation"
      ],
      "metadata": {
        "id": "Z81xKG12ozfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BB8eNpZcoycI"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_community langchain_google_genai chroma sentence_transformers pypdf faiss-gpu rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Single PDF File using PyPDF Loader"
      ],
      "metadata": {
        "id": "XWrS_rodputK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"./langchian.pdf\")\n",
        "docs = loader.load()\n",
        "docs[0]"
      ],
      "metadata": {
        "id": "OAzuYhy-pQEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].metadata) #print Metadata like filename"
      ],
      "metadata": {
        "id": "Jw_I7CmVpe8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Mulitple PDF File using PyPDFDirectory Loader"
      ],
      "metadata": {
        "id": "ghpzsRgRp19v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "loader = PyPDFDirectoryLoader(\"./example_data\") #folder path\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "jAPkUDDBpm9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading CSV File using CSVLoader"
      ],
      "metadata": {
        "id": "1ODjGqvGp5fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "loader = CSVLoader(file_path=\"./data.csv\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "kpyX8qXJpRc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZk8Ob1U8DZK"
      },
      "source": [
        "### Split Documents and Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYZ1j1Ns73mU"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "#Splitting data\n",
        "text_split = RecursiveCharacterTextSplitter(chunk_size=500,\n",
        "                                          chunk_overlap=50)\n",
        "chunks = text_split.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Total count of splitted chunks\n",
        "print(len(chunks))"
      ],
      "metadata": {
        "id": "b6XEe0PAwDXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the chuck data from the index\n",
        "chunks[0]"
      ],
      "metadata": {
        "id": "WNtyDvGmwBmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Model"
      ],
      "metadata": {
        "id": "6-WSFiWkDwj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name = 'BAAI/bge-large-en-v1.5',  #load different model from huggingface if needed.\n",
        "                                   model_kwargs={'device':\"cuda\"}          #set 'cpu' if GPU not available.\n",
        "                                   )"
      ],
      "metadata": {
        "id": "QKpdurCEwF5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding chunk data and storing it Vectorstore"
      ],
      "metadata": {
        "id": "4VXOS6fTrczo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "qzSxYcXfrRzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving locally and loading from local"
      ],
      "metadata": {
        "id": "IBadVU3OxfLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore.save_local(index_dir)\n",
        "vector_db = FAISS.load_local(index_dir, embeddings, allow_dangerous_deserialization=True)"
      ],
      "metadata": {
        "id": "uwkJFb81xdA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Gemini LLM model"
      ],
      "metadata": {
        "id": "7lAVhsyBrvIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import os\n",
        "GOOGLE_API_KEY = \"API Key\" # replace your API token\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "imrGShAirk2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0) #Play with parameters if you needed.\n",
        "# Temperature controls randomness in generating text '0' to '1' is high."
      ],
      "metadata": {
        "id": "fqH5grsxruoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat Memory"
      ],
      "metadata": {
        "id": "iLxL4qYht57h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ],
      "metadata": {
        "id": "CanqV9int5vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Templates from langchain HUB"
      ],
      "metadata": {
        "id": "OdvMh7S8svTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RAG Prompt\n",
        "from langchain import hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "ENRRyQUOseSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Prompt Template"
      ],
      "metadata": {
        "id": "P21phSS7s0Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "#you can re-write and play with it. as you required.\n",
        "template = \"\"\"\n",
        "<|system|>>\n",
        "You are a helpful AI Assistant that follows instructions extremely well.\n",
        "Answer the following question form the given context.\n",
        "\n",
        "CONTEXT: {context}\n",
        "</s>\n",
        "<|user|>\n",
        "{query}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "BSyBmI9Dr2Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid Search"
      ],
      "metadata": {
        "id": "r58kXRX4NlGQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMucCA4z9VAn"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "keyword_retriever = BM25Retriever.from_documents(chunks)\n",
        "keyword_retriever.k =  3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3UCqmV5-D-r"
      },
      "source": [
        "### Ensemble Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVhLxoWh95dv"
      },
      "outputs": [],
      "source": [
        "ensemble_retriever = EnsembleRetriever(retrievers=[vectorstore_retreiver,\n",
        "                                                   keyword_retriever],\n",
        "                                       weights=[0.5, 0.5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OutputParser"
      ],
      "metadata": {
        "id": "yghJzsemyCp_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8eDFcNd_Kru"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "output_parser = StrOutputParser()                        #helps to get output in str format\n",
        "# output_parser = CommaSeparatedListOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting cache"
      ],
      "metadata": {
        "id": "SmDp2MXZyKfa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOg_GTIWkF1W"
      },
      "outputs": [],
      "source": [
        "from langchain.globals import set_llm_cache\n",
        "from langchain.cache import InMemoryCache\n",
        "set_llm_cache(InMemoryCache())      #reduce reteriving for same queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyRq7bg-kF1W"
      },
      "source": [
        "<h3>Pipeline</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JtvKJk2_Ok2"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain = (\n",
        "    {\"context\": ensemble_retriever, \"query\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | output_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(f\"Eplain Agents in Langchain\")"
      ],
      "metadata": {
        "id": "_NVSvzEcyfWk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}