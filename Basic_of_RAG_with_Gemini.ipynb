{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Requirements Installation"
      ],
      "metadata": {
        "id": "Z81xKG12ozfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BB8eNpZcoycI"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_community langchain_google_genai chroma sentence_transformers pypdf faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Single PDF File using PyPDF Loader"
      ],
      "metadata": {
        "id": "XWrS_rodputK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\"./langchian.pdf\")\n",
        "docs = loader.load()\n",
        "docs[0]"
      ],
      "metadata": {
        "id": "OAzuYhy-pQEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].metadata) #print Metadata like filename"
      ],
      "metadata": {
        "id": "Jw_I7CmVpe8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Mulitple PDF File using PyPDFDirectory Loader"
      ],
      "metadata": {
        "id": "ghpzsRgRp19v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "loader = PyPDFDirectoryLoader(\"./example_data\") #folder path\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "jAPkUDDBpm9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading CSV File using CSVLoader"
      ],
      "metadata": {
        "id": "1ODjGqvGp5fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "loader = CSVLoader(file_path=\"./data.csv\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "kpyX8qXJpRc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZk8Ob1U8DZK"
      },
      "source": [
        "### Split Documents and Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYZ1j1Ns73mU"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "#Splitting data\n",
        "text_split = RecursiveCharacterTextSplitter(chunk_size=500,\n",
        "                                          chunk_overlap=50)\n",
        "chunks = text_split.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Total count of splitted chunks\n",
        "print(len(chunks))"
      ],
      "metadata": {
        "id": "b6XEe0PAwDXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print the chuck data from the index\n",
        "chunks[0]"
      ],
      "metadata": {
        "id": "WNtyDvGmwBmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Model"
      ],
      "metadata": {
        "id": "6-WSFiWkDwj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name = 'BAAI/bge-large-en-v1.5',  #load different model from huggingface if needed.\n",
        "                                   model_kwargs={'device':\"cuda\"}          #set 'cpu' if GPU not available.\n",
        "                                   )"
      ],
      "metadata": {
        "id": "QKpdurCEwF5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding chunk data and storing it Vectorstore"
      ],
      "metadata": {
        "id": "4VXOS6fTrczo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "qzSxYcXfrRzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Gemini LLM model"
      ],
      "metadata": {
        "id": "7lAVhsyBrvIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "GOOGLE_API_KEY = \"API Key\" # replace your API token\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "imrGShAirk2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0) #Play with parameters if you needed.\n",
        "# Temperature controls randomness in generating text '0' to '1' is high."
      ],
      "metadata": {
        "id": "fqH5grsxruoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat Memory"
      ],
      "metadata": {
        "id": "iLxL4qYht57h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ],
      "metadata": {
        "id": "CanqV9int5vE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Templates from langchain HUB"
      ],
      "metadata": {
        "id": "OdvMh7S8svTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RAG Prompt\n",
        "from langchain import hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "ENRRyQUOseSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Prompt Template"
      ],
      "metadata": {
        "id": "P21phSS7s0Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "#you can re-write and play with it. as you required.\n",
        "template = \"\"\"\n",
        "<|system|>>\n",
        "You are a helpful AI Assistant that follows instructions extremely well.\n",
        "Answer the following question form the given context.\n",
        "\n",
        "CONTEXT: {context}\n",
        "</s>\n",
        "<|user|>\n",
        "{question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "BSyBmI9Dr2Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieval Chain"
      ],
      "metadata": {
        "id": "tAe6kaz6JTqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "fWarsXBkJEWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RAG Pipeline\n",
        "chain = RetrievalQA.from_chain_type(llm=llm, #loading Our Model\n",
        "                                 chain_type='stuff',\n",
        "                                 return_source_documents=True, # return source filename where it retrive data from\n",
        "                                 retriever= vector_db.as_retriever(), #vectorDB\n",
        "                                 chain_type_kwargs={'prompt': prompt, # pormpt template\n",
        "                                                    #'memory':memory # Chat Memory if needed\n",
        "                                                    }\n",
        "                                 )"
      ],
      "metadata": {
        "id": "xumWeKRQJESv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain({\"query\": \"What is langchian?\"})\n",
        "result = response['result']\n",
        "print(result)"
      ],
      "metadata": {
        "id": "1zdd06I4ulRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  user_query = input(\"Enter your query: \")\n",
        "  if user_query == \"exit\":\n",
        "    break\n",
        "  response = chain({\"query\": user_query})\n",
        "  result = response['result']\n",
        "  print(result)"
      ],
      "metadata": {
        "id": "uPGIVuGuvOTW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}